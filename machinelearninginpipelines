# What it takes to use machine learning in fast data pipelines

https://deanwampler.github.io/polyglotprogramming/papers/ExecutiveBriefing-WhatItTakesToUseMLinFastDataPipelines.pdf

## Batch vs Streaming

  - Real time updates 

## Data science vs data engineering

  - Seperated tool sets between data eng vs science
  - Data scientists
   - Comfortable with uncertainty
   - Used to statistics
   - Less process oriented
     - Iterative , experimental
  - Data Engineers
   - Uncomfortable with uncertainty
   - Process oriented 
     - "Agile"
     - Doesn't mention data

## Where to use AI First

  - Hybrid systems - enhance existing analytics with ML/AI Models
  - Show we integrate legacy expert systems and how?
  
  
## Serving models in production

  - Rough 
  - Kube flow
  - Challenges 
    - https://www.kubeflow.org/
    - Integration is a bit difficult
    - Concerns:
      - Productivity
      - Devops challenges
  - Needs
    - VC for modesl and code
    - Automated builds, tests, qc, and artifact delievery
    - Launch confurations: "dark launches", A/B and other testing scenarios
    - Monitoring: Performance overhead
      - Much slower 
      - Quality metrics amtch expectations from training?
    - Auditing 
      - Which model used to score this record
      - Which records used to train this model
      - Who accessed this model and when
      - Metadata storage 
   - Indetermancy in to the system
   
## Complete systems for ml/ai

  - Model seving as a service See slide for architecture
    - Important part:
       - Isolated components to create training
       - Isolated component to use the training model at the time of processing
       - Decouples Ai system
    - Loss of data service
  - Embedding learning tool (Tensor flow serving)
    - Heavy on the communication between tensorflow and models 
    - Faster on processing because it's a microcluster
    - Lowest overhead
    
## Stuff

  - Kubeflow - K8
  - SageMaker - for aws users
  - MLFlow from Spark community

## Auditing 
 - What you want to track
 - Kind of model
 - Parameters and hyper parameters
 - When trained 
 - Data used for training 
 - Quality metrics
 - Serving metrics 
 - Provenance of decision to retrain - metrics gathered aove that where used to decide when to retrain
   
## Pragmatic challenges 

## Reference later 

 - Automatic Rule learning?
 - Embedded rules to influence learning
   - Pretrain the model 
 - 9 ai trends
 - Paco nathans ai talk data governnence
 - distil.pub https://distill.pub/
 - The algorithm
 - Fiddler - explain what the model is doing
 
## Questions?

 - REPL
 - Testing 
 - Seperation of data science vs engineering and where does that merge?
 - Integrate legact expert systems and how: Relearning rules?
 - In the concerns, are you looking at systems that are adapting live or from static models?

 - How do you store the models?
 - How do you version the models that you have for new updates?

Git repo, database .. although complex parameters are needed?
Not high volume outputs


KubeFlow 
Has model meta data (awesome!) 


 - How do you test/debug models in production

 Auditing 
 
 - How do you manage situations where you may need to reprocess due to bad data?
 - Stability of the systems?
 
All about data quality
Models are the problems
- Quality assurance 
- This is A HUGE thing to be aware of 

